from langchain.prompts.prompt import PromptTemplate
from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores.base import VectorStoreRetriever
#from langchain.chains import ChatVectorDBChain
import os

_template = """Учитывая следующий разговор и последующий вопрос, перефразируйте последующий вопрос так, чтобы он был отдельным вопросом.
Вы можете предположить, что речь идет о последнем запросе о сервисе "Продано"..

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:"""
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)

template = """Вы - ИИ помощник, отвечающий на вопросы о сервисе "Продано".
Вам даны следующие извлеченные части длинного документа и вопрос. Дайте разговорный ответ.
Если вы не знаете ответа, просто скажите: "Хм, я не уверен". Не пытайтесь придумать ответ.
Если вопрос не касается самой последней информации об услуге "Продано", вежливо сообщите, что вы настроены отвечать только на вопросы о сервисе "Продано"..
Question: {question}
=========
{context}
=========
Answer in Markdown:"""
QA_PROMPT = PromptTemplate(template=template, input_variables=["question", "context"])


def get_chain(vectorstore):
    llm = OpenAI(temperature=0, openai_api_key='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')  # Don't forget to add your OpenAI API Key
    retriever = VectorStoreRetriever(vectorstore=vectorstore)  # Create a retriever instance
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever,
        condense_question_prompt=CONDENSE_QUESTION_PROMPT,
        chain_type='stuff',
        combine_docs_chain_kwargs={},  # If additional arguments are required, add them here
    )
    return qa_chain
